[package]
name = "screenpipe-core"
version = { workspace = true }
authors = { workspace = true }
description = { workspace = true }
repository = { workspace = true }
license = { workspace = true }
edition = { workspace = true }

[dependencies]
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
which = "6.0.1"
log = "0.4.17"
anyhow = "1.0.86"

# pipes
reqwest = { workspace = true }
tokio = { workspace = true }

# Security
regex = { version = "1.10.6", features = ["std"], optional = true }
lazy_static = { version = "1.4.0", optional = true }
tempfile = "3.3.0"
url = "2.4.0"

tracing = { workspace = true }
tracing-subscriber = { workspace = true }

dirs = "5.0.0"

# ai
llama-cpp-2 = { git = "https://github.com/utilityai/llama-cpp-rs.git", optional = true, features = ["metal"] }
candle = { workspace = true, optional = true }
candle-nn = { workspace = true, optional = true }
candle-transformers = { workspace = true, optional = true }
tokenizers = { workspace = true, optional = true }
hf-hub = { workspace = true, features = ["tokio"], optional = true }

[features]
default = ["pipes", "security"]
llm = ["candle", "candle-nn", "candle-transformers", "tokenizers", "hf-hub", "llama-cpp-2"]
pipes = []
security = ["dep:regex", "dep:lazy_static"]
metal = ["candle/metal", "candle-nn/metal", "candle-transformers/metal", "llama-cpp-2/metal"]
cuda = ["candle/cuda", "candle-nn/cuda", "candle-transformers/cuda", "llama-cpp-2/cuda"]
mkl = ["candle/mkl", "candle-nn/mkl", "candle-transformers/mkl"]

[build-dependencies]
cc = "1.1.30"

